{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Processing Example with Neuraxle\n",
    "The goal of the present tutorial is to show how usage of the Neuraxle framework can make a difference in term of clean pipeline design through an example of time series processing.\n",
    "\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "It'll be downloaded automatically for you in the code below. \n",
    "\n",
    "We're using a Human Activity Recognition (HAR) dataset captured using smartphones. The [dataset](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) can be found on the UCI Machine Learning Repository. \n",
    "\n",
    "### The task\n",
    "\n",
    "Classify the type of movement amongst six categories from the phones' sensor data:\n",
    "- WALKING,\n",
    "- WALKING_UPSTAIRS,\n",
    "- WALKING_DOWNSTAIRS,\n",
    "- SITTING,\n",
    "- STANDING,\n",
    "- LAYING.\n",
    "\n",
    "### Video dataset overview\n",
    "\n",
    "Follow this link to see a video of the 6 activities recorded in the experiment with one of the participants:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=XOEN9W05_4A\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/XOEN9W05_4A/0.jpg\" \n",
    "alt=\"Video of the experiment\" width=\"400\" height=\"300\" border=\"10\" /></a>\n",
    "  <a href=\"https://youtu.be/XOEN9W05_4A\"><center>[Watch video]</center></a>\n",
    "</p>\n",
    "\n",
    "### Details about the input data\n",
    "\n",
    "The dataset's description goes like this:\n",
    "\n",
    "> The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. \n",
    "\n",
    "Reference: \n",
    "> Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.\n",
    "\n",
    "That said, I will use the almost raw data: only the gravity effect has been filtered out of the accelerometer  as a preprocessing step for another 3D feature as an input to help learning. If you'd ever want to extract the gravity by yourself, you could use the following [Butterworth Low-Pass Filter (LPF)](https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform) and edit it to have the right cutoff frequency of 0.3 Hz which is a good frequency for activity recognition from body sensors.\n",
    "\n",
    "Here is how the 3D data cube looks like. So we'll have a train and a test data cube, and might create validation data cubes as well: \n",
    "\n",
    "![](time-series-data.jpg)\n",
    "\n",
    "So we have 3D data of shape `[batch_size, time_steps, features]`. If this and the above is still unclear to you, you may want to [learn more on the 3D shape of time series data](https://www.quora.com/What-do-samples-features-time-steps-mean-in-LSTM/answers/79038267).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 - Loading the Dataset\n",
    "This first part downloads and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "Downloading done.\n",
      "\n",
      "Extracting...\n",
      "Extracting successfully done to /home/gui/Documents/Neuraxio/Marketing/talks/AiHubspot 2021/data/UCI HAR Dataset.\n",
      "\n",
      "Dataset is now located at: data/UCI HAR Dataset/\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import os\n",
    "from subprocess import call\n",
    "\n",
    "def download_dataset_if_needed():\n",
    "    print(\"Downloading...\")\n",
    "    if not os.path.exists(\"UCI HAR Dataset.zip\"):\n",
    "        call(\n",
    "            'wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"',\n",
    "            shell=True\n",
    "        )\n",
    "        print(\"Downloading done.\\n\")\n",
    "    else:\n",
    "        print(\"Dataset already downloaded. Did not download twice.\\n\")\n",
    "\n",
    "\n",
    "    print(\"Extracting...\")\n",
    "    extract_directory = os.path.abspath(\"UCI HAR Dataset\")\n",
    "    if not os.path.exists(extract_directory):\n",
    "        call(\n",
    "            'unzip -nq \"UCI HAR Dataset.zip\"',\n",
    "            shell=True\n",
    "        )\n",
    "        print(\"Extracting successfully done to {}.\".format(extract_directory))\n",
    "    else:\n",
    "        print(\"Dataset already extracted. Did not extract twice.\\n\")\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "if not os.path.exists(DATA_PATH): os.mkdir(DATA_PATH)\n",
    "os.chdir(DATA_PATH)\n",
    "download_dataset_if_needed()\n",
    "os.chdir(\"..\")\n",
    "DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
    "print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import neuraxle\n",
    "    assert neuraxle.__version__ == '0.5.8'\n",
    "except:\n",
    "    !pip install neuraxle==0.5.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Useful Constants\n",
    "DATA_PATH = \"data/\"\n",
    "DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\",\n",
    "    \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\",\n",
    "    \"SITTING\",\n",
    "    \"STANDING\",\n",
    "    \"LAYING\"\n",
    "]\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_all_data():\n",
    "    # Load \"X\" (the neural network's training and testing inputs)\n",
    "\n",
    "    def load_X(X_signals_paths):\n",
    "        X_signals = []\n",
    "\n",
    "        for signal_type_path in X_signals_paths:\n",
    "            file = open(signal_type_path, 'r')\n",
    "            # Read dataset from disk, dealing with text files' syntax\n",
    "            X_signals.append(\n",
    "                [np.array(serie, dtype=np.float32) for serie in [\n",
    "                    row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "                ]]\n",
    "            )\n",
    "            file.close()\n",
    "\n",
    "        return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "    X_train_signals_paths = [\n",
    "        DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "    ]\n",
    "    X_test_signals_paths = [\n",
    "        DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "    ]\n",
    "\n",
    "    X_train = load_X(X_train_signals_paths)\n",
    "    X_test = load_X(X_test_signals_paths)\n",
    "\n",
    "    def load_y(y_path):\n",
    "        file = open(y_path, 'r')\n",
    "        # Read dataset from disk, dealing with text file's syntax\n",
    "        y_ = np.array(\n",
    "            [elem for elem in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]],\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        file.close()\n",
    "\n",
    "        # Substract 1 to each output class for friendly 0-based indexing\n",
    "        return y_ - 1\n",
    "\n",
    "    y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "    y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "    y_train = load_y(y_train_path)\n",
    "    y_test = load_y(y_test_path)\n",
    "\n",
    "    print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "    print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "    print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(2947, 128, 9) (2947, 1) 0.09913992 0.39567086\n",
      "Dataset loaded!\n"
     ]
    }
   ],
   "source": [
    "# Finally load dataset!\n",
    "X_train, y_train, X_test, y_test = load_all_data()\n",
    "print(\"Dataset loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - How would you code this in a typical ML project using Scikit-learn\n",
    "\n",
    "We'll first define functions that will extract features from the 3d data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def get_fft_peak_infos(real_fft, time_bins_axis=-2):\n",
    "    \"\"\"\n",
    "    Extract the indices of the bins with maximal amplitude, and the corresponding amplitude values.\n",
    "\n",
    "    :param fft: real magnitudes of an fft. It could be of shape [N, bins, features].\n",
    "    :param time_bins_axis: axis of the frequency bins (e.g.: time axis before fft).\n",
    "    :return: Two arrays without bins. One is an int, the other is a float. Shape: ([N, features], [N, features])\n",
    "    \"\"\"\n",
    "    peak_bin = np.argmax(real_fft, axis=time_bins_axis)\n",
    "    peak_bin_val = np.max(real_fft, axis=time_bins_axis)\n",
    "    return peak_bin, peak_bin_val\n",
    "\n",
    "\n",
    "def fft_magnitudes(data_inputs, time_axis=-2):\n",
    "    \"\"\"\n",
    "    Apply a Fast Fourier Transform operation to analyze frequencies, and return real magnitudes.\n",
    "    The bins past the half (past the nyquist frequency) are discarded, which result in shorter time series.\n",
    "\n",
    "    :param data_inputs: ND array of dimension at least 1. For instance, this could be of shape [N, time_axis, features]\n",
    "    :param time_axis: axis along which the time series evolve\n",
    "    :return: real magnitudes of the data_inputs. For instance, this could be of shape [N, (time_axis / 2) + 1, features]\n",
    "             so here, we have `bins = (time_axis / 2) + 1`.\n",
    "    \"\"\"\n",
    "    fft = np.fft.rfft(data_inputs, axis=time_axis)\n",
    "    real_fft = np.absolute(fft)\n",
    "    return real_fft\n",
    "\n",
    "\n",
    "def get_fft_features(x_data):\n",
    "    \"\"\"\n",
    "    Will featurize data with an FFT.\n",
    "\n",
    "    :param x_data: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "    :return: featurized time series with FFT of shape [batch_size, features]\n",
    "    \"\"\"\n",
    "    real_fft = fft_magnitudes(x_data)\n",
    "    flattened_fft = real_fft.reshape(real_fft.shape[0], -1)\n",
    "    peak_bin, peak_bin_val = get_fft_peak_infos(real_fft)\n",
    "    return flattened_fft, peak_bin, peak_bin_val\n",
    "\n",
    "\n",
    "def featurize_data(x_data):\n",
    "    \"\"\"\n",
    "    Will convert 3D time series of shape [batch_size, time_steps, sensors] to shape [batch_size, features]\n",
    "    to prepare data for machine learning.\n",
    "\n",
    "    :param x_data: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "    :return: featurized time series of shape [batch_size, features]\n",
    "    \"\"\"\n",
    "    print(\"Input shape before feature union:\", x_data.shape)\n",
    "\n",
    "    flattened_fft, peak_bin, peak_bin_val = get_fft_features(x_data)\n",
    "    mean = np.mean(x_data, axis=-2)\n",
    "    median = np.median(x_data, axis=-2)\n",
    "    min = np.min(x_data, axis=-2)\n",
    "    max = np.max(x_data, axis=-2)\n",
    "\n",
    "    featurized_data = np.concatenate([\n",
    "        flattened_fft,\n",
    "        peak_bin,\n",
    "        peak_bin_val,\n",
    "        mean,\n",
    "        median,\n",
    "        min,\n",
    "        max,\n",
    "    ], axis=-1)\n",
    "\n",
    "    print(\"Shape after feature union, before classification:\", featurized_data.shape)\n",
    "    return featurized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before feature union: (7352, 128, 9)\n",
      "Shape after feature union, before classification: (7352, 639)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape: [batch_size, time_steps, sensor_features]\n",
    "X_train_featurized = featurize_data(X_train)\n",
    "# Shape: [batch_size, remade_features]\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train_featurized, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before feature union: (2947, 128, 9)\n",
      "Shape after feature union, before classification: (2947, 639)\n",
      "Shape at output after classification: (2947,)\n",
      "Accuracy of sklearn pipeline code: 0.8642687478791992\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Shape: [batch_size, time_steps, sensor_features]\n",
    "X_test_featurized = featurize_data(X_test)\n",
    "# Shape: [batch_size, remade_features]\n",
    "\n",
    "y_pred = classifier.predict(X_test_featurized)\n",
    "print(\"Shape at output after classification:\", y_pred.shape)\n",
    "# Shape: [batch_size]\n",
    "accuracy = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "print(\"Accuracy of sklearn pipeline code:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - How to code a similar pipeline - but cleaner - using Neuraxle\n",
    "\n",
    "To make ourselves a cleaner pipeline, we'll define each of our transformation as steps. Defining our pipeline in term of steps allows us to implement separately the behaviour on *fit* calls and on *transform* calls. In our present case though, we only need to define a *transform* behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuraxle.base import BaseStep, NonFittableMixin\n",
    "from neuraxle.steps.numpy import NumpyConcatenateInnerFeatures, NumpyShapePrinter, NumpyFlattenDatum, NumpyRavel\n",
    "\n",
    "class NumpyStep(NonFittableMixin, BaseStep):\n",
    "    def __init__(self):\n",
    "        BaseStep.__init__(self)\n",
    "        NonFittableMixin.__init__(self)\n",
    "\n",
    "class NumpyFFT(NumpyStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Featurize time series data with FFT.\n",
    "\n",
    "        :param data_inputs: time series data of 3D shape: [batch_size, time_steps, sensors_readings]\n",
    "        :return: featurized data is of 2D shape: [batch_size, n_features]\n",
    "        \"\"\"\n",
    "        transformed_data = np.fft.rfft(data_inputs, axis=-2)\n",
    "        return transformed_data\n",
    "\n",
    "\n",
    "class FFTPeakBinWithValue(NumpyStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will compute peak fft bins (int), and their magnitudes' value (float), to concatenate them.\n",
    "\n",
    "        :param data_inputs: real magnitudes of an fft. It could be of shape [batch_size, bins, features].\n",
    "        :return: Two arrays without bins concatenated on feature axis. Shape: [batch_size, 2 * features]\n",
    "        \"\"\"\n",
    "        time_bins_axis = -2\n",
    "        peak_bin = np.argmax(data_inputs, axis=time_bins_axis)\n",
    "        peak_bin_val = np.max(data_inputs, axis=time_bins_axis)\n",
    "        \n",
    "        # Notice that here another FeatureUnion could have been used with a joiner:\n",
    "        transformed = np.concatenate([peak_bin, peak_bin_val], axis=-1)\n",
    "        \n",
    "        return transformed\n",
    "\n",
    "\n",
    "class NumpyAbs(NumpyStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a max.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.abs(data_inputs)\n",
    "\n",
    "\n",
    "class NumpyMean(NumpyStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a mean.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.mean(data_inputs, axis=-2)\n",
    "\n",
    "\n",
    "class NumpyMedian(NumpyStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a median.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.median(data_inputs, axis=-2)\n",
    "\n",
    "\n",
    "class NumpyMin(NumpyStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a min.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.min(data_inputs, axis=-2)\n",
    "\n",
    "\n",
    "class NumpyMax(NumpyStep):\n",
    "    def transform(self, data_inputs):\n",
    "        \"\"\"\n",
    "        Will featurize data with a max.\n",
    "\n",
    "        :param data_inputs: 3D time series of shape [batch_size, time_steps, sensors]\n",
    "        :return: featurized time series of shape [batch_size, features]\n",
    "        \"\"\"\n",
    "        return np.max(data_inputs, axis=-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a set of classifier we'd like to test and their respective hyperparameter space we'd like to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuraxle.hyperparams.distributions import Choice, Boolean\n",
    "from neuraxle.hyperparams.distributions import RandInt, LogUniform\n",
    "from neuraxle.hyperparams.space import HyperparameterSpace\n",
    "from neuraxle.pipeline import Pipeline\n",
    "from neuraxle.steps.output_handlers import OutputTransformerWrapper\n",
    "from neuraxle.steps.sklearn import SKLearnWrapper\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "\n",
    "decision_tree_classifier = SKLearnWrapper(\n",
    "    DecisionTreeClassifier(), \n",
    "    HyperparameterSpace({\n",
    "        'criterion': Choice(['gini', 'entropy']), \n",
    "        'splitter': Choice(['best', 'random']),\n",
    "        'min_samples_leaf': RandInt(2, 5), \n",
    "        'min_samples_split': Choice([0.5, 1.0, 2, 3]), \n",
    "})).set_name('DecisionTreeClassifier')\n",
    "\n",
    "extra_tree_classifier = SKLearnWrapper(\n",
    "    ExtraTreeClassifier(), \n",
    "    HyperparameterSpace({\n",
    "        'criterion': Choice(['gini', 'entropy']), \n",
    "        'splitter': Choice(['best', 'random']),\n",
    "        'min_samples_leaf': RandInt(2, 5), \n",
    "        'min_samples_split': Choice([0.5, 1.0, 2, 3]), \n",
    "})).set_name('ExtraTreeClassifier')\n",
    "\n",
    "ridge_classifier = Pipeline([OutputTransformerWrapper(NumpyRavel()), SKLearnWrapper(\n",
    "    RidgeClassifier(), \n",
    "    HyperparameterSpace({\n",
    "        'alpha': Choice([0.0, 1.0, 10.0]), \n",
    "        'fit_intercept': Boolean(), \n",
    "        'normalize': Boolean()\n",
    "    }))\n",
    "]).set_name('RidgeClassifier')\n",
    "\n",
    "logistic_regression = Pipeline([OutputTransformerWrapper(NumpyRavel()), SKLearnWrapper(\n",
    "    LogisticRegression(), \n",
    "    HyperparameterSpace({\n",
    "        'C': LogUniform(0.01, 10.0), \n",
    "        'fit_intercept': Boolean(),\n",
    "        'penalty': Choice(['none', 'l2']), \n",
    "        'max_iter': RandInt(20, 200)\n",
    "    }))\n",
    "]).set_name('LogisticRegression')\n",
    "\n",
    "random_forest_classifier = Pipeline([OutputTransformerWrapper(NumpyRavel()), SKLearnWrapper(\n",
    "    RandomForestClassifier(), \n",
    "    HyperparameterSpace({\n",
    "        'n_estimators': RandInt(50, 600), \n",
    "        'criterion': Choice(['gini', 'entropy']),\n",
    "        'min_samples_leaf': RandInt(2, 5), \n",
    "        'min_samples_split': Choice([0.5, 1.0, 2, 3]),\n",
    "        'bootstrap': Boolean()\n",
    "    }))\n",
    "]).set_name('RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the pieces to define a proper Neuraxle Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuraxle.steps.flow import TrainOnlyWrapper, ChooseOneStepOf\n",
    "from neuraxle.steps.numpy import NumpyConcatenateInnerFeatures, NumpyShapePrinter, NumpyFlattenDatum\n",
    "from neuraxle.union import FeatureUnion\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    TrainOnlyWrapper(NumpyShapePrinter(custom_message=\"Input shape before feature union\")),\n",
    "    FeatureUnion([\n",
    "        Pipeline([\n",
    "            NumpyFFT(),\n",
    "            NumpyAbs(),\n",
    "            FeatureUnion([\n",
    "                NumpyFlattenDatum(),  # Reshape from 3D to flat 2D: flattening data except on batch size\n",
    "                FFTPeakBinWithValue()  # Extract 2D features from the 3D FFT bins\n",
    "            ], joiner=NumpyConcatenateInnerFeatures())\n",
    "        ]),\n",
    "        NumpyMean(),\n",
    "        NumpyMedian(),\n",
    "        NumpyMin(),\n",
    "        NumpyMax()\n",
    "    ], joiner=NumpyConcatenateInnerFeatures()),\n",
    "    TrainOnlyWrapper(NumpyShapePrinter(custom_message=\"Shape after feature union, before classification\")),\n",
    "    # Shape: [batch_size, remade_features]\n",
    "    ChooseOneStepOf([\n",
    "        decision_tree_classifier,\n",
    "        extra_tree_classifier,\n",
    "        ridge_classifier,\n",
    "        logistic_regression,\n",
    "        random_forest_classifier\n",
    "    ]),\n",
    "    TrainOnlyWrapper(NumpyShapePrinter(custom_message=\"Shape at output after classification\")),\n",
    "    # Shape: [batch_size]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our pipeline, we can give it to an AutoML loop which will explore the hyperparameter space for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil \n",
    "\n",
    "# Clear cache if we've already ran the AutoML to start fresh:\n",
    "cache_folder = 'cache'\n",
    "if os.path.exists(cache_folder):\n",
    "    shutil.rmtree(cache_folder)\n",
    "os.makedirs(cache_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuraxle.metaopt.auto_ml import AutoML, InMemoryHyperparamsRepository, ValidationSplitter, \\\n",
    "    RandomSearchHyperparameterSelectionStrategy\n",
    "#from neuraxle.metaopt.tpe import TreeParzenEstimatorSelectionStrategy\n",
    "#from neuraxle.metaopt.auto_ml import HyperparamsJSONRepository\n",
    "from neuraxle.metaopt.callbacks import ScoringCallback\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "auto_ml = AutoML(\n",
    "    pipeline=pipeline,\n",
    "    hyperparams_optimizer=RandomSearchHyperparameterSelectionStrategy(),\n",
    "    validation_splitter=ValidationSplitter(test_size=0.20),\n",
    "    scoring_callback=ScoringCallback(accuracy_score, higher_score_is_better=True),\n",
    "    n_trials=10,\n",
    "    epochs=1,\n",
    "    hyperparams_repository=InMemoryHyperparamsRepository(cache_folder=cache_folder),\n",
    "    refit_trial=True,\n",
    "    # callbacks=[MetricCallbacks(...)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:14:24][INFO][auto_ml][246]: \n",
      "new trial: {\n",
      "    \"ChooseOneStepOf\": {\n",
      "        \"DecisionTreeClassifier\": {\n",
      "            \"Optional(DecisionTreeClassifier)\": {\n",
      "                \"criterion\": \"entropy\",\n",
      "                \"min_samples_leaf\": 3,\n",
      "                \"min_samples_split\": 0.5,\n",
      "                \"splitter\": \"random\"\n",
      "            }\n",
      "        },\n",
      "        \"ExtraTreeClassifier\": {\n",
      "            \"Optional(ExtraTreeClassifier)\": {\n",
      "                \"criterion\": \"entropy\",\n",
      "                \"min_samples_leaf\": 4,\n",
      "                \"min_samples_split\": 2,\n",
      "                \"splitter\": \"best\"\n",
      "            }\n",
      "        },\n",
      "        \"LogisticRegression\": {\n",
      "            \"Optional(LogisticRegression)\": {\n",
      "                \"SKLearnWrapper_LogisticRegression\": {\n",
      "                    \"C\": 0.0734038830915426,\n",
      "                    \"fit_intercept\": false,\n",
      "                    \"max_iter\": 52,\n",
      "                    \"penalty\": \"none\"\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"RandomForestClassifier\": {\n",
      "            \"Optional(RandomForestClassifier)\": {\n",
      "                \"SKLearnWrapper_RandomForestClassifier\": {\n",
      "                    \"bootstrap\": true,\n",
      "                    \"criterion\": \"entropy\",\n",
      "                    \"min_samples_leaf\": 3,\n",
      "                    \"min_samples_split\": 2,\n",
      "                    \"n_estimators\": 255\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"RidgeClassifier\": {\n",
      "            \"Optional(RidgeClassifier)\": {\n",
      "                \"SKLearnWrapper_RidgeClassifier\": {\n",
      "                    \"alpha\": 0.0,\n",
      "                    \"fit_intercept\": false,\n",
      "                    \"normalize\": true\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"choice\": \"ExtraTreeClassifier\"\n",
      "    }\n",
      "}\n",
      "[12:14:24][INFO][auto_ml][862]: trial 1/10\n",
      "[12:14:24][INFO][auto_ml][590]: fitting trial 1/10 split 1/1\n",
      "hyperparams: {\n",
      "    \"ChooseOneStepOf\": {\n",
      "        \"DecisionTreeClassifier\": {\n",
      "            \"Optional(DecisionTreeClassifier)\": {\n",
      "                \"ccp_alpha\": 0.0,\n",
      "                \"class_weight\": null,\n",
      "                \"criterion\": \"entropy\",\n",
      "                \"max_depth\": null,\n",
      "                \"max_features\": null,\n",
      "                \"max_leaf_nodes\": null,\n",
      "                \"min_impurity_decrease\": 0.0,\n",
      "                \"min_impurity_split\": null,\n",
      "                \"min_samples_leaf\": 3,\n",
      "                \"min_samples_split\": 0.5,\n",
      "                \"min_weight_fraction_leaf\": 0.0,\n",
      "                \"random_state\": null,\n",
      "                \"splitter\": \"random\"\n",
      "            },\n",
      "            \"enabled\": false\n",
      "        },\n",
      "        \"ExtraTreeClassifier\": {\n",
      "            \"Optional(ExtraTreeClassifier)\": {\n",
      "                \"ccp_alpha\": 0.0,\n",
      "                \"class_weight\": null,\n",
      "                \"criterion\": \"entropy\",\n",
      "                \"max_depth\": null,\n",
      "\n",
      "\n",
      "\n",
      "[...]\n",
      "[...]\n",
      "[...]\n",
      "\n",
      "\n",
      "\n",
      "    },\n",
      "    \"TrainOnlyWrapper2\": {\n",
      "        \"NumpyShapePrinter\": {}\n",
      "    }\n",
      "}\n",
      "[12:19:42][INFO][auto_ml][630]: epoch 1/1\n",
      "NumpyShapePrinter: (5881, 128, 9) Input shape before feature union\n",
      "NumpyShapePrinter: (5881, 639) Shape after feature union, before classification\n",
      "[12:19:43][INFO][trial][450]: main train: 0.5279714334296888\n",
      "[12:19:43][INFO][trial][471]: main validation: 0.5248130523453433\n",
      "[12:19:43][INFO][auto_ml][605]: success trial 10/10 split 1/1\n",
      "hyperparams: {\n",
      "    \"ChooseOneStepOf\": {\n",
      "        \"DecisionTreeClassifier\": {\n",
      "            \"Optional(DecisionTreeClassifier)\": {\n",
      "                \"ccp_alpha\": 0.0,\n",
      "                \"class_weight\": null,\n",
      "                \"criterion\": \"entropy\",\n",
      "                \"max_depth\": null,\n",
      "                \"max_features\": null,\n",
      "                \"max_leaf_nodes\": null,\n",
      "                \"min_impurity_decrease\": 0.0,\n",
      "                \"min_impurity_split\": null,\n",
      "                \"min_samples_leaf\": 3,\n",
      "                \"min_samples_split\": 3,\n",
      "                \"min_weight_fraction_leaf\": 0.0,\n",
      "                \"random_state\": null,\n",
      "                \"splitter\": \"random\"\n",
      "            },\n",
      "            \"enabled\": false\n",
      "        },\n",
      "        \"ExtraTreeClassifier\": {\n",
      "            \"Optional(ExtraTreeClassifier)\": {\n",
      "                \"ccp_alpha\": 0.0,\n",
      "                \"class_weight\": null,\n",
      "                \"criterion\": \"gini\",\n",
      "                \"max_depth\": null,\n",
      "                \"max_features\": \"auto\",\n",
      "                \"max_leaf_nodes\": null,\n",
      "                \"min_impurity_decrease\": 0.0,\n",
      "                \"min_impurity_split\": null,\n",
      "                \"min_samples_leaf\": 5,\n",
      "                \"min_samples_split\": 0.5,\n",
      "                \"min_weight_fraction_leaf\": 0.0,\n",
      "                \"random_state\": null,\n",
      "                \"splitter\": \"best\"\n",
      "            },\n",
      "            \"enabled\": true\n",
      "        },\n",
      "        \"LogisticRegression\": {\n",
      "            \"Optional(LogisticRegression)\": {\n",
      "                \"OutputTransformerWrapper\": {\n",
      "                    \"NumpyRavel\": {}\n",
      "                },\n",
      "                \"SKLearnWrapper_LogisticRegression\": {\n",
      "                    \"C\": 8.175969462206426,\n",
      "                    \"class_weight\": null,\n",
      "                    \"dual\": false,\n",
      "                    \"fit_intercept\": true,\n",
      "                    \"intercept_scaling\": 1,\n",
      "                    \"l1_ratio\": null,\n",
      "                    \"max_iter\": 178,\n",
      "                    \"multi_class\": \"auto\",\n",
      "                    \"n_jobs\": null,\n",
      "                    \"penalty\": \"l2\",\n",
      "                    \"random_state\": null,\n",
      "                    \"solver\": \"lbfgs\",\n",
      "                    \"tol\": 0.0001,\n",
      "                    \"verbose\": 0,\n",
      "                    \"warm_start\": false\n",
      "                }\n",
      "            },\n",
      "            \"enabled\": false\n",
      "        },\n",
      "        \"RandomForestClassifier\": {\n",
      "            \"Optional(RandomForestClassifier)\": {\n",
      "                \"OutputTransformerWrapper\": {\n",
      "                    \"NumpyRavel\": {}\n",
      "                },\n",
      "                \"SKLearnWrapper_RandomForestClassifier\": {\n",
      "                    \"bootstrap\": false,\n",
      "                    \"ccp_alpha\": 0.0,\n",
      "                    \"class_weight\": null,\n",
      "                    \"criterion\": \"entropy\",\n",
      "                    \"max_depth\": null,\n",
      "                    \"max_features\": \"auto\",\n",
      "                    \"max_leaf_nodes\": null,\n",
      "                    \"max_samples\": null,\n",
      "                    \"min_impurity_decrease\": 0.0,\n",
      "                    \"min_impurity_split\": null,\n",
      "                    \"min_samples_leaf\": 4,\n",
      "                    \"min_samples_split\": 2,\n",
      "                    \"min_weight_fraction_leaf\": 0.0,\n",
      "                    \"n_estimators\": 133,\n",
      "                    \"n_jobs\": null,\n",
      "                    \"oob_score\": false,\n",
      "                    \"random_state\": null,\n",
      "                    \"verbose\": 0,\n",
      "                    \"warm_start\": false\n",
      "                }\n",
      "            },\n",
      "            \"enabled\": false\n",
      "        },\n",
      "        \"RidgeClassifier\": {\n",
      "            \"Optional(RidgeClassifier)\": {\n",
      "                \"OutputTransformerWrapper\": {\n",
      "                    \"NumpyRavel\": {}\n",
      "                },\n",
      "                \"SKLearnWrapper_RidgeClassifier\": {\n",
      "                    \"alpha\": 10.0,\n",
      "                    \"class_weight\": null,\n",
      "                    \"copy_X\": true,\n",
      "                    \"fit_intercept\": false,\n",
      "                    \"max_iter\": null,\n",
      "                    \"normalize\": true,\n",
      "                    \"random_state\": null,\n",
      "                    \"solver\": \"auto\",\n",
      "                    \"tol\": 0.001\n",
      "                }\n",
      "            },\n",
      "            \"enabled\": false\n",
      "        },\n",
      "        \"choice\": \"ExtraTreeClassifier\",\n",
      "        \"joiner\": {}\n",
      "    },\n",
      "    \"FeatureUnion\": {\n",
      "        \"NumpyMax\": {},\n",
      "        \"NumpyMean\": {},\n",
      "        \"NumpyMedian\": {},\n",
      "        \"NumpyMin\": {},\n",
      "        \"Pipeline\": {\n",
      "            \"FeatureUnion\": {\n",
      "                \"FFTPeakBinWithValue\": {},\n",
      "                \"NumpyFlattenDatum\": {},\n",
      "                \"joiner\": {}\n",
      "            },\n",
      "            \"NumpyAbs\": {},\n",
      "            \"NumpyFFT\": {}\n",
      "        },\n",
      "        \"joiner\": {}\n",
      "    },\n",
      "    \"TrainOnlyWrapper\": {\n",
      "        \"NumpyShapePrinter\": {}\n",
      "    },\n",
      "    \"TrainOnlyWrapper1\": {\n",
      "        \"NumpyShapePrinter\": {}\n",
      "    },\n",
      "    \"TrainOnlyWrapper2\": {\n",
      "        \"NumpyShapePrinter\": {}\n",
      "    }\n",
      "}\n",
      "best score: 0.5248130523453433 at epoch 0\n",
      "[12:19:43][INFO][auto_ml][832]: \n",
      "best hyperparams: {\n",
      "    \"ChooseOneStepOf\": {\n",
      "        \"DecisionTreeClassifier\": {\n",
      "            \"Optional(DecisionTreeClassifier)\": {\n",
      "                \"ccp_alpha\": 0.0,\n",
      "                \"class_weight\": null,\n",
      "                \"criterion\": \"gini\",\n",
      "                \"max_depth\": null,\n",
      "                \"max_features\": null,\n",
      "                \"max_leaf_nodes\": null,\n",
      "                \"min_impurity_decrease\": 0.0,\n",
      "                \"min_impurity_split\": null,\n",
      "                \"min_samples_leaf\": 2,\n",
      "                \"min_samples_split\": 0.5,\n",
      "                \"min_weight_fraction_leaf\": 0.0,\n",
      "                \"random_state\": null,\n",
      "                \"splitter\": \"random\"\n",
      "            },\n",
      "            \"enabled\": false\n",
      "        },\n",
      "        \"ExtraTreeClassifier\": {\n",
      "            \"Optional(ExtraTreeClassifier)\": {\n",
      "                \"ccp_alpha\": 0.0,\n",
      "                \"class_weight\": null,\n",
      "                \"criterion\": \"entropy\",\n",
      "                \"max_depth\": null,\n",
      "                \"max_features\": \"auto\",\n",
      "                \"max_leaf_nodes\": null,\n",
      "                \"min_impurity_decrease\": 0.0,\n",
      "                \"min_impurity_split\": null,\n",
      "                \"min_samples_leaf\": 4,\n",
      "                \"min_samples_split\": 1.0,\n",
      "                \"min_weight_fraction_leaf\": 0.0,\n",
      "                \"random_state\": null,\n",
      "                \"splitter\": \"best\"\n",
      "            },\n",
      "            \"enabled\": false\n",
      "        },\n",
      "        \"LogisticRegression\": {\n",
      "            \"Optional(LogisticRegression)\": {\n",
      "                \"OutputTransformerWrapper\": {\n",
      "                    \"NumpyRavel\": {}\n",
      "                },\n",
      "                \"SKLearnWrapper_LogisticRegression\": {\n",
      "                    \"C\": 6.020923240680797,\n",
      "                    \"class_weight\": null,\n",
      "                    \"dual\": false,\n",
      "                    \"fit_intercept\": false,\n",
      "                    \"intercept_scaling\": 1,\n",
      "                    \"l1_ratio\": null,\n",
      "                    \"max_iter\": 98,\n",
      "                    \"multi_class\": \"auto\",\n",
      "                    \"n_jobs\": null,\n",
      "                    \"penalty\": \"none\",\n",
      "                    \"random_state\": null,\n",
      "                    \"solver\": \"lbfgs\",\n",
      "                    \"tol\": 0.0001,\n",
      "                    \"verbose\": 0,\n",
      "                    \"warm_start\": false\n",
      "                }\n",
      "            },\n",
      "            \"enabled\": true\n",
      "        },\n",
      "        \"RandomForestClassifier\": {\n",
      "            \"Optional(RandomForestClassifier)\": {\n",
      "                \"OutputTransformerWrapper\": {\n",
      "                    \"NumpyRavel\": {}\n",
      "                },\n",
      "                \"SKLearnWrapper_RandomForestClassifier\": {\n",
      "                    \"bootstrap\": true,\n",
      "                    \"ccp_alpha\": 0.0,\n",
      "                    \"class_weight\": null,\n",
      "                    \"criterion\": \"entropy\",\n",
      "                    \"max_depth\": null,\n",
      "                    \"max_features\": \"auto\",\n",
      "                    \"max_leaf_nodes\": null,\n",
      "                    \"max_samples\": null,\n",
      "                    \"min_impurity_decrease\": 0.0,\n",
      "                    \"min_impurity_split\": null,\n",
      "                    \"min_samples_leaf\": 2,\n",
      "                    \"min_samples_split\": 3,\n",
      "                    \"min_weight_fraction_leaf\": 0.0,\n",
      "                    \"n_estimators\": 510,\n",
      "                    \"n_jobs\": null,\n",
      "                    \"oob_score\": false,\n",
      "                    \"random_state\": null,\n",
      "                    \"verbose\": 0,\n",
      "                    \"warm_start\": false\n",
      "                }\n",
      "            },\n",
      "            \"enabled\": false\n",
      "        },\n",
      "        \"RidgeClassifier\": {\n",
      "            \"Optional(RidgeClassifier)\": {\n",
      "                \"OutputTransformerWrapper\": {\n",
      "                    \"NumpyRavel\": {}\n",
      "                },\n",
      "                \"SKLearnWrapper_RidgeClassifier\": {\n",
      "                    \"alpha\": 1.0,\n",
      "                    \"class_weight\": null,\n",
      "                    \"copy_X\": true,\n",
      "                    \"fit_intercept\": false,\n",
      "                    \"max_iter\": null,\n",
      "                    \"normalize\": true,\n",
      "                    \"random_state\": null,\n",
      "                    \"solver\": \"auto\",\n",
      "                    \"tol\": 0.001\n",
      "                }\n",
      "            },\n",
      "            \"enabled\": false\n",
      "        },\n",
      "        \"choice\": \"LogisticRegression\",\n",
      "        \"joiner\": {}\n",
      "    },\n",
      "    \"FeatureUnion\": {\n",
      "        \"NumpyMax\": {},\n",
      "        \"NumpyMean\": {},\n",
      "        \"NumpyMedian\": {},\n",
      "        \"NumpyMin\": {},\n",
      "        \"Pipeline\": {\n",
      "            \"FeatureUnion\": {\n",
      "                \"FFTPeakBinWithValue\": {},\n",
      "                \"NumpyFlattenDatum\": {},\n",
      "                \"joiner\": {}\n",
      "            },\n",
      "            \"NumpyAbs\": {},\n",
      "            \"NumpyFFT\": {}\n",
      "        },\n",
      "        \"joiner\": {}\n",
      "    },\n",
      "    \"TrainOnlyWrapper\": {\n",
      "        \"NumpyShapePrinter\": {}\n",
      "    },\n",
      "    \"TrainOnlyWrapper1\": {\n",
      "        \"NumpyShapePrinter\": {}\n",
      "    },\n",
      "    \"TrainOnlyWrapper2\": {\n",
      "        \"NumpyShapePrinter\": {}\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumpyShapePrinter: (7352, 128, 9) Input shape before feature union\n",
      "NumpyShapePrinter: (7352, 639) Shape after feature union, before classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "auto_ml = auto_ml.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.9365456396335257\n"
     ]
    }
   ],
   "source": [
    "best_pipeline = auto_ml.get_best_model()\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Test accuracy score:\", accuracy)\n",
    "assert accuracy > 0.9, \"Try again harder!\"\n",
    "# It's getting good on this dataset if you're over 92%. The current code is able to do this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
